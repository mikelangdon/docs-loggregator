---
title: Key Performance Indicators 
owner: PCF Metrics Platform Monitoring
---

This topic describes how to use Key Performance Indicators (KPIs) to monitor your Pivotal Cloud Foundry (PCF) 
deployment and ensure it is in a good operational state. 

## <a id="overview"></a> Overview

<table>
   <tr>
      <th width="35%">For metrics related to the component&hellip;</th>
      <th>and the source&hellip;</th>
      <th>see&hellip;</th>
   <tr>
   </tr>
       <td>Diego</td>
       <td>Auctioneer</td>
       <td><a href="#auctioneer">Link.</a></td>
   </tr>
   </tr>
       <td>Diego</td>
       <td>BBS</td>
       <td><a href="#bbs">Link.</a></td>
   </tr>
   </tr>
       <td>Diego</td>
       <td>Cell</td>
       <td><a href="#cell">Link.</a></td>
   </tr>
   </tr>
       <td>Diego</td>
       <td>nsync_bulker</td>
       <td><a href="#nsync_bulker">Link.</a></td>
   </tr>
   </tr>
       <td>Diego</td>
       <td>router-emitter</td>
       <td><a href="#route_emitter">Link.</a></td>
   </tr>
   </tr>
       <td>Gorouter</td>
       <td>Gorouter</td>
       <td><a href="#gorouter">Link.</a></td>
   </tr>
   </tr>
       <td>Doppler Server</td>
       <td><em>N/A</em></td>
       <td><a href="#doppler-server">Link.</a></td>
   </tr>
   </tr>
       <td>System (BOSH)</td>
       <td><em>N/A</em></td>
       <td><a href="#bosh">Link.</a></td>
   </tr>
</table>

## <a id="auctioneer"></a> Diego Auctioneer Metrics
 
<a name="AuctioneerLRPAuctionsFailed"></a>

<table>
   <tr><th colspan="2" style="text-align: center;"><br> auctioneer.AuctioneerLRPAuctionsFailed<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The cumulative number of Long Running Processes (LRP) instances that the auctioneer failed to place on Diego cells <br><br>
   
      <strong>Use</strong>: This metric can indicates that PCF is out of container space or that there is a lack of resources within your environment.
   
      This error is most common due to capacity issues, for example, if cells do not have 
      enough resources or if cells are blipping in and out of health.
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: Counter, integer<br>
      <strong>Frequency</strong>: During each auction<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Per minute average over a 5-minute window</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &ge; 0.5<br>
      <strong>Red critical</strong>: &ge; 1</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
         <ol>
            <li>Investigate the health of your Diego cells to determine if they
            are the resource type causing the problem.</li> 
            <li>Consider scaling additional cells using Ops Manager.</li>
            <li>If scaling cells does not solve the problem, pull Diego brain logs and BBS logs
             and contact Pivotal Support indicating that LRP auctions are failing.</li>
         </ol>
      </td>
   </tr>
</table>

Q: We need to standardize on the adjectives for VM: brain VM, Brain VM, Diego-related VM, 
BBS VM, cell VM, Router VM, router VM, Gorouter VM, and route_emitter VM. 

A: router and Router are Gorouter: DONE
A: cell and Cell and Diego Cell are probably all the same.
A: instance and VM are the same.
A: brain and Diego brain and Diego Brain are all the same. We standardize on "Diego brain".
A: route-emitter (not Route Emitter or route_emitter)

Some are the same and some are different but I don't know for sure.

Q: Then without the word VM, does it still mean VM? cells, Diego cells, 

<a name="AuctioneerFetchStatesDuration"></a>

<table>
   <tr><th colspan="2" style="text-align: center;"><br>auctioneer.AuctioneerFetchStatesDuration<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td> Time in ns that the auctioneer took to fetch state from all the cells when running its auction.<br><br>

      <strong>Use</strong>: Indicates how the cells themselves are performing. 
      Alerting on this metric helps alert that app staging requests to Diego may be failing.

      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: Gauge, integer in ns<br>
      <strong>Frequency</strong>: During event, during each auction<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td> Maximum over the last 5 minutes divided by 1,000,000,000</td>
   </tr>
   <tr>
         <th>Recommended alert thresholds</th>
         <td><strong>Yellow warning</strong>: &ge; 5 s<br>
         <strong>Red critical</strong>: &ge; 10 s</td>
   </tr>
   <tr>
      <th>Recommended response</th>
        <td>
          <ol>
             <li>Check the health of the cells by reviewing the logs and looking for errors.</li>
             <li>Review IaaS console metrics.</li>
             <li>Pull Diego brain logs and cell logs and contact Pivotal Support indicating that fetching cell states is taking too long.</li>
          </ol>
       </td>
   </tr>
</table>

<a name="AuctioneerLRPAuctionsStarted"></a>
<table> 
   <tr><th colspan="2" style="text-align: center;"><br>auctioneer.AuctioneerLRPAuctionsStarted<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Cumulative number of LRP instances that the auctioneer successfully placed on Diego cells.
                        <br><br>
                <strong>Use</strong>: Provides a sense of running system activity levels in your environment. 
                Can also give you a sense of how many app instances have been started over time.
                The recommended measurement, below, can help indicate a lot of container churn. 
                However, for capacity planning purposes, it is more helpful to observe deltas over a long time window. 
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Integer)<br>
                 <strong>Frequency</strong>: During event, during each auction<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Per minute average of <code>auctioneer.AuctioneerLRPAuctionsStarted</code> averaged over a 5-minute window</td>
</tr>
<tr>
<th>Recommended alert thresholds</th>
<td><strong>Yellow warning</strong>: Dynamic<br>
<strong>Red critical</strong>: Dynamic</td>
</tr>
<tr>
<th>Recommended response</th>
<td>
When observing a lot of container churn, do the following:<br><br>
    <ol>
    <li>Look to eliminate explainable causes of temporary churn, such as a deployment or a lot of developer activity</li>
    <li>For extended periods of high or low activity, consider scaling up or down CF components as necessary.</li>
    </ol>
    </td>
</tr>
</table>

Q for Eric Malm: What does cumulative mean here? When does this metric get reset? 
(Does it mean "The number of LRP instances that the auctioneer
successfully placed on Diego cells in one auction? or since when?)

<a name="AuctioneerTaskAuctionsFailed"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>auctioneer.AuctioneerTaskAuctionsFailed<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Cumulative number of Tasks that the auctioneer failed to placed on Diego cells.
                        <br><br>
                <strong>Use</strong>: Failing Task auctions indicate a lack of resources 
                        within your environment and that you likely need to scale. 
                        This indicator also increases when the Task is requesting an isolation segment, volume drivers, 
                        or a stack that is unavailable, either not deployed or lacking in sufficient resources to accept the work.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Float)<br>
                 <strong>Frequency</strong>: During event, during each auction<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Per minute average of <code>auctioneer.AuctioneerTaskAuctionsFailed</code> averaged over a 5-minute window</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 0.5 <br>
                <strong>Red critical</strong>: &ge; 1</td>
        </tr>
<tr>
<th>Recommended response</th>
<td>
    <ol>
    <li>Investigate the health of Diego cells.</li>
    <li>Consider scaling additional cells using Ops Manager.</li>
    <li>If scaling cells doesn't solve the problem, 
                        then pull Diego brain logs and BBS logs for troubleshooting, 
                        and contact Pivotal Support indicating that Task auctions are failing.</li>
    </ol>
    </td>
</tr>
</table>

Q: I've written "Task" with a capital T everywhere. These are the Tasks that siblings of LRPs. Okay?

## <a id="bbs"></a> Diego BBS Metrics

<a name="ConvergenceLRPDuration"></a>
<table>
   <tr><th colspan="2" style="text-align: center;"><br>bbs.ConvergenceLRPDuration<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Time in ns that the BBS took to run its LRP convergence pass. 
          <br><br>
          <strong>Use</strong>: If the convergence run begins taking too long, 
          maybe apps or Tasks are crashing and not being restarted. 
          This symptom can also indicate loss of connectivity to the BBS database.
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Integer in ns)<br>
          <strong>Frequency</strong>: During event,
                  every 30 seconds when LRP convergence runs, emission should be constant on a running, Diego-based, PCF deployment
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>15-minute maximum of <code>bbs.ConvergenceLRPDuration</code> divided by 1,000,000,000</td>
   </tr>
      <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &ge; 10 s<br>
          <strong>Red critical</strong>: &ge; 20 s</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          <ol>
              <li>Check BBS logs for errors.</li>
              <li>Try vertically scaling the BBS VM resources up. 
              For example, add more CPUs/memory depending on its system.cpu/system.memory metrics.</li>
              <li>If that does not solve the issue, pull the BBS logs and contact Pivotal Support 
                  for additional troubleshooting.
          </ol>
          </td>
   </tr>
</table>


<a name="RequestLatency"></a>
<table>
   <tr><th colspan="2" style="text-align: center;"><br>bbs.RequestLatency<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Time in ns that the BBS took to handle requests, aggregated across all its API endpoints. 
          <br><br>
          <strong>Use</strong>: If this rises, it indicates that the PCF API is slowing. 
          Response to certain "cf" commands is slow if request latency is high.
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Integer in ns)<br>
          <strong>Frequency</strong>: During event, when the BBS API handles requests, 
                  emission should be constant on a running, Diego-based, PCF deployment<br>
    </tr>
    <tr>
      <th>Recommended measurement</th>
      <td>15 minute maximum of bbs.RequestLatency divided by 1,000,000,000</td>
    </tr>
    <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &ge; 5 s<br>
          <strong>Red critical</strong>: &ge; 10 s</td>
    </tr>
    <tr>
      <th>Recommended response</th>
      <td>
          <ol>
              <li>Check CPU and memory statistics in Ops Manager.</li>
              <li>Check BBS logs for faults and errors that can indicate issues with BBS.</li>
              <li>Try scaling the BBS VM resources up. For example, add more CPUs/memory 
                  depending on its system.cpu/system.memory metrics.</li>
              <li>If the above steps do not solve the issue, then collect a sample of the cell logs 
                  from the BBS VMs and contact Pivotal Support to troubleshoot further.
          </ol>
          </td>
    </tr>
</table>


<a name="Domain.cf-apps"></a>
<table>
   <tr><th colspan="2" style="text-align: center;"><br>bbs.Domain.cf-apps<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>`1` means the <code>cf-apps</code> domain is up-to-date, so that the CF apps from the Cloud Controller
           are synchronized with DesiredLRPs for Diego to run. Emmitted periodically.
           `0` means that the 'cf-apps' domain is not up-to-date.
          <br><br>
          <strong>Use</strong>: cf-apps is used to translate Cloud Controller desired state into Diego implementation. 
          Without freshness, changes in Cloud Controller state is not guaranteed to propagate through the system. 
          If Cloud Controller and Diego are out of sync, the running apps can stray 
          from the set of desired apps.
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: <br>
          <strong>Frequency</strong>: 30 s<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over the last 5 minutes of <code>bbs.Domain.cf\_apps</code></td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: <em>N/A</em> <br>
      <strong>Red critical</strong>: &lt; 1 </td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          <ol>
              <li>Check the nysnc bulker logs and BBS logs.</li>
              <li>If the problem continues, pull Diego brain logs and BBS logs 
                  and contact Pivotal Support to say that cf domain is not being kept fresh.</li>
          </ol>
      </td>
   </tr>
</table>

Q for Eric Malm: What are "CF apps"? How are they different from the apps that run on PCF deployments? 

<a name="LRPsExtra"></a>

Q: I think much of the table above needs a rewrite in language that our customers will easily understand.

Q: The master table says that the emit frequency is 30 s but also that it is "Emitted periodically". Can we just say every 30 s?

<table>
     <tr><th colspan="2" style="text-align: center;"><br>bbs.LRPsExtra<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Total number of LRP instances that are no longer desired but still have a BBS record. 
                    When Diego wants to add more apps, the BBS sends a request to the auctioneer 
                    to spin up additional LRPs. 
                        <br><br>
                <strong>Use</strong>: If Diego has more LRP running than expected, 
                        there might be problems with the BBS.<br>
                        Deleting an app with many instances can temporarily spike this metric.  
                        However, a sustained spike in <code>bbs.LRPsExtra</code> is unusual and should be investigated.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 5 minutes of <code>bbs.LRPsExtra</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 5<br>
                <strong>Red critical</strong>:  &ge; 10</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Review the BBS logs for proper operation or errors, looking for detailed error messages.</li>
                        <li>If the condition persists, pull the BBS logs and contact Pivotal Support.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="LRPsMissing"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>bbs.LRPsMissing<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Total number of LRP instances that are desired but have no record in the BBS.
                        <br><br>
                <strong>Use</strong>: Diego has less LRP running than expected. 
                        When Diego wants to add more apps, the BBS sends a request to the auctioneer 
                        to spin up additional LRPs. 
                        LRPsMissing is the total number of LRP instances that are desired but have no BBS record. 
                        This can indicate issues with the BBS.<br> 
                        An app push with many instances can temporarily spike this metric. 
                        However, a sustained spike in <code>bbs.LRPsMissing</code> is unusual and should be investigated.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 5 minutes of <code>bbs.LRPsMissing</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 5<br>
                <strong>Red critical</strong>: &ge; 10</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Review the BBS logs for proper operation or errors, looking for detailed error messages.</li>
                        <li>If the condition persists, pull the BBS logs and contact Pivotal Support.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="bbs.CrashedActualLRPs"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>bbs.CrashedActualLRPs<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Total number of LRP instances that have crashed.
                <br><br>
                <strong>Use</strong>: Indicates how many instances in the deployment are in a crashed state. 
                An increase in bbs.CrashedActualLRPs can indicate several problems, from a bad app with many instances associated, 
                to a platform issue that is resulting in app crashes. 
                This metric is most helpful to create a baseline in a deployment. 
                Once a baseline is created, a deployment specific alert can be created to notify of a spike in crashes above trend line. 
                Alert values should be tuned to the deployment. 
                <br><br>
                <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over last 5 minutes of <code>bbs.CrashedActualLRPs</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Look at the BBS logs for apps that are crashing 
                            and at the cell logs to see if the problem is with the apps themselves, rather than 
                            a platform issue.</li>
                        <li>Before contacting Pivotal Support, pull the BBS logs and, if particular apps are the problem,
                            pull the logs from their cells too.</li>
                    </ol>
                    </td>
        </tr>
</table>

<a name="1hraverageofbbs.LRPsRunning"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>1hr average of bbs.LRPsRunning - prior 1hr average of bbs.LRPsRunning<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Rate of change in app instances being started or stopped on the platform. 
                    It is derived from <code>bbs.LRPsRunning</code>
                    and represents the total number of LRP instances that are running on cells.
                    <br><br>
                    <strong>Use</strong>: Delta reflects upward or downward trend for app instances started or stopped. 
                    Helps to provide a picture of the overall growth trend of the environment for capacity planning. 
                    May want to alert on delta values out of expected range.
                    <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: During event, 
                         emission should be constant on a running, Diego-based, PCF deployment<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>derived=(1-hour average of <code>bbs.LRPsRunning</code> - prior 1-hour average of <code>bbs.LRPsRunning</code>)</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                   <ol>
                     <li>Scale up down CF components as necessary.</li>
                   </ol>
                    </td>
        </tr>
</table>



## <a id="cell"></a> Diego Cell Metrics

<a name="rep.CapacityRemainingMemory"></a>
<table>
   <tr><th colspan="2" style="text-align: center;"><br>rep.CapacityRemainingMemory<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Remaining amount in MiB of memory available for this cell to allocate to containers.
          <br><br>
          <strong>Use</strong>: Indicates the available cell memory. 
          If low memory, this could prevent app scaling and new deployments. 
          It is common to look at this metric as both a minimum value as well as a sum capacity across all cells. 
          Minimum value over time of this metric has more informational value than alerting value. 
          It can be an interesting heatmap, showing average variance and/or density over time. 
          Sum capacity can also be an interesting indicator of the need to potentially scale an environment. 
          However, the greater operational value is to understand a deployments average app size, 
          and monitor/alert on ensuring that at least some cells have large enough capacity to accept standard app size pushes. 
          For example, if pushing a 4&nbsp;GB app, Diego would have trouble placing that app if there is no one cell with sufficient capacity. 
          If deployment average was 3.9&nbsp;GB free, yet no one cell contained at least 4&nbsp;GB, the app push would fail. 
          As an example, Pivotal Cloud Ops uses a standard of 4&nbsp;GB, and computes and monitors 
          for the number of cells with at least 4&nbsp;GB free. 
          When the number of cells with at least 4&nbsp;GB falls below a defined threshold, 
          this is a scaling indicator alert to add additional capacity. 
          This "Free 4GB Chunk" count threshold should be tuned to the deployment size 
          and the standard size of apps being pushed to the platform.<br>
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Integer in bytes)<br>
          <strong>Frequency</strong>: 30 s<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Minimum amount of memory in a 30-minute window that the cell can allocate to containers.<br><br>
          <strong>Implementation Tip</strong>: Iterate through each Diego cell and grab the 
          <code>rep.CapacityRemainingMemory</code> metric.<br><br>
          Divide this metric by 1000 to get the value in GB. 
          Then compare it to your minimum capacity threshold, and count the number of cells that have enough free space.</td>
  </tr>
  <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Dynamic<br>
          <strong>Red critical</strong>: Dynamic</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          <ol>
              <li>Assign more resources to the cells or assign more cells.</li>
                        <li>Scale additional Diego cells using Ops Manager.</li>
                    </ol>
                    </td>
        </tr>
</table>

<a name="rep.CapacityRemainingMemory2"></a>
<table>
   <tr><th colspan="2" style="text-align: center;"><br>rep.CapacityRemainingMemory<br>
       (Alternative Use)<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Remaining amount in MiB of memory available for this cell to allocate to containers.
          <br><br>
          <strong>Use</strong>: Can be used to indicate low memory capacity overall in the platform. 
          Low memory can prevent app scaling and new deployments. 
          Sum capacity overall is helpful to understanding the need to take action to scale the platform. 
          Observing how capacity consumption trends over time is useful for ongoing capacity planning.
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Integer in bytes)<br>
          <strong>Frequency</strong>: 30 s<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Sum of the total available memory capacity across across all cells for the past 5 minutes<br>
             5 minute sum of <code>rep.CapacityRemainingMemory</code> divided by 0.000001024</td>
  </tr>
  <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 32<br>
          <strong>Red critical</strong>: &le; 16</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          <ol>
              <li>Assign more resources to the cells or assign more cells</li>
              <li>Scale additional Diego cells via Ops Manager</li>
          </ol>
                    </td>
        </tr>
</table>
<a name="CapacityRemainingDisk"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br> rep.CapacityRemainingDisk<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Remaining amount in MiB of disk available for this cell to allocate to containers.
                    <br><br>
                    <strong>Use</strong>: Indicates the available cell disk. 
                Low disk capacity could prevent app scaling and new deployments.
                Similar to rep.CapacityRemainingMemory, it can be meaningful to assess how many chunks of free disk space are above a given threshold. 
                As Diego staging Tasks can fail without at least 4GB free, this recommend red 
                threshold is based on the minimum disk capacity 
                across the deployment falling below 4GB in the prior 5 minutes.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Integer in bytes)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Minimum over the last 5 minutes of <code>rep.CapacityRemainingDisk</code> divided by 1024 (across all instances)</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &le; 6<br>
                <strong>Red critical</strong>:&le; 3.5 </td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Assign more resources to the cells or assign more cells</li>
                        <li>Scale additional cells using Ops Manager.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="RepBulkSyncDuration"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>rep.RepBulkSyncDuration<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in ns that the Cell Rep took to synchronize the ActualLRPs it has claimed with its actual garden containers. 
                        <br><br>
                <strong>Use</strong>: Sync times that are too high may indicate issues with the BBS.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ns)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum over the last 15 minutes of <code>rep.RepBulkSyncDuration</code> divided by 1,000,000,000</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 10 s<br>
                <strong>Red critical</strong>: &ge; 20 s </td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Investigate BBS logs for faults and errors.</li>
                        <li>If a particular cell or cells look to be problematic, 
                            pull logs for that cell, as well as the BBS logs before contacting Pivotal Support.</li>
                    </ol>
                    </td>
        </tr>
</table>

Q: Are all "cells" "Diego cells"?

<a name="UnhealthyCell"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>rep.UnhealthyCell<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The cell periodically checks its health against the garden backend. 
                    When it fails this healthcheck, it emits a 1 (unhealthy).
                    For Diego cells, 0 signifies healthy, and 1 signifies unhealthy.
                    <br><br>
                    <strong>Use</strong>: Should alert for further investigation if 
                    multiple unhealthy Diego cells are detected in the given time window. 
                    If one cell is impacted, it does not participate in auctions, 
                    but customer impact is usually low. If multiple cells are impacted, 
                    this can indicate a larger problem with Diego.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float, 0-1)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum over the last 5 minutes of <code>rep.UnhealthyCell</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: <em>N/A</em><br>
                <strong>Red critical</strong>: &gt; 1 </td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Investigate Diego cell servers for faults and errors.</li>
                        <li>If a particular cell or cells look to be problematic, 
                            pull logs for that cell, as well as the BBS logs before contacting Pivotal Support.</li>
                    </ol>
                    </td>
        </tr>
</table>


## <a id="nsync_bulker"></a> Diego nsync_bulker Metrics

<a name="DesiredLRPSyncDuration"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>nsync\_bulker.DesiredLRPSyncDuration<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in ns that the nsync-bulker took to synchronize CF apps and Diego DesiredLRPs.
                    <br><br>
                    <strong>Use</strong>: Cloud Controller and Diego brain should be kept synchronized. 
                    This symptom can indicate that the BBS database is in an unhealthy state. 
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ns)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum over the last 15 minutes of <code>nsync\_bulker.DesiredLRPSyncDuration</code> 
                    divided by 1,000,000,000</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 10 s<br>
                <strong>Red critical</strong>: &ge; 20 s</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Investigate BBS metrics and logs and Cloud Controller Bridge logs for errors.
                    </td>
        </tr>
</table>

##<a id="route_emitter"></a>Diego router-emitter Metrics

<a name="RouteEmitterSyncDuration"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>route\_emitter.RouteEmitterSyncDuration<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in ns that the active route-emitter took to perform its synchronization pass. 
                    <br><br>
                    <strong>Use</strong>: Increases in this metric indicate that the route emitter may be having 
                    trouble maintaining an accurate routing table to broadcast to the Gorouters. 
                    Alerting values need to be tuned per the deployment, based on historical data adjusted based on observations over time. 
                    The suggested starting point is &ge; 10 for the yellow threshold and &ge; 20 for the critical threshold. 
                    Pivotal has observed on Pivotal Web Services that above 20 s, the BBS may be failing.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ns)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum over the last 15 minutes of <code>route\_emitter.RouteEmitterSyncDuration</code> divided by 1,000,000,000</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Investigate the routing table and Diego brains for errors.
                </td>
        </tr>
</table>

Q: How do customers investigate "Diego brains"? The docs show only one hit for this term and it is an architectural diagram. 

<a name="ConsulDownMode"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>route\_emitter.ConsulDownMode<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>`1` means the system is healthy, and `0` means that route emitter detects that consul is down.
                    <br><br>
                    <strong>Use</strong>: During installation or upgrade, this can indicate that consul is down.
                    Loss of the consul cluster results in apps becoming unavailable 
                    because their routes were pruned from the routing table.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float, 0-1)<br>
                 <strong>Frequency</strong>: Periodically, emission should be constant on a running, Diego-based, PCF deployment<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 5 minutes of <code>route\_emitter.ConsulDownMode</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: <em>N/A</em><br>
                <strong>Red critical</strong>: Not equal to 0 </td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>During upgrade, reduce the consul server nodes to 1 and then upgrade.</li>
                        <li>Outside of installation or upgrading, consider disaster recovery for consul.</li>
                    </ol>
                    </td>
        </tr>
</table>

Q for Eric Malm: What action is recommended if you see this during installation? 
The recommended response addresses the upgrade scenario and non-install/upgrade 
but no mention of what to do if this happens during installation.
## <a id="gorouter"></a> Gorouter Metrics

<a name="total_requests"></a>
<table>
   <tr><th colspan="2" style="text-align: center;"><br>gorouter.total\_requests<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The lifetime (since the Gorouter VM started) number of requests completed by component 
          <br><br>
          <strong>Use</strong>: Provides insight into the overall traffic flow through a deployment. 
          For performance and capacity management, consider this metric a measure of router throughput 
          and convert it to "requests per second", by deriving <code>per\_second(sum:gorouter.total\_requests{*})</code>.
          This helps you see trends in throughput rate that indicate a need to scale the Gorouter.
          Use the trends you observe to tune the threshold alerts for this metric. 
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Counter (Integer)<br>
          <strong>Frequency</strong>: 5 s<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over the last 5 minutes of the per second calculation of <code>gorouter.total\_requests</code></td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Dynamic<br>
          <strong>Red critical</strong>: Dynamic</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>For optimizing the Gorouter, consider the "requests per second" 
          derived metric in the context of router latency and Gorouter VM CPU utilization.
          In testing the Gorouter, Pivotal observed that at approximately 2500 requests 
          per second, latency increases. <br><br>
          To increase throughput, while keeping latency low, 
          scale the Gorouters, either horizontally or vertically, 
          while watching the Gorouter CPU utilization metric.
   </tr>
</table>

Q: Should it be turned into a derived metric from the start? 

<a name="latency"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.latency<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The time in milliseconds that the Gorouter takes to handle requests to its app endpoints. 
                    This is the average round trip response time to an app, which includes router handling. 
                    <br><br>
                    <strong>Use</strong>: Indicates how Gorouter jobs in PCF are impacting overall app responsiveness. 
                    Latencies above 100ms can indicate problems with the network, misbehaving apps, 
                    or the need to scale the Gorouter itself due to ongoing traffic congestion. 
                    An alert value on this metric should be tuned to the specifics of the deployment; 
                    a suggested starting point is 100ms. 
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ms)<br>
                 <strong>Frequency</strong>: Emitted per Gorouter request,
                         emission should be constant on a running, Diego-based, PCF deployment
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 30 minutes of <code>gorouter.latency</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>Extended periods of high latency can point to several factors. 
                    The Gorouter latency measure includes network and app latency impacts as well.<br><br>
                    <ol>
                        <li>First inspect logs for network issues and indications of misbehaving apps.</li>
                        <li>Even if it appears that the Gorouter needs to scale due to ongoing traffic congestion, 
                            do not scale on the latency metric alone. 
                            Also look at the CPU utilization of the Gorouter VMs.</li>
                        <li>Keep CPU utilization within a maximum 60-70% range.</li>
                        <li>Resolve high utilization by scaling the Gorouter.</li>
                        <li>To increase throughput, while keeping latency low, 
                            scale the Gorouters, either horizontally or vertically, 
                            while watching the Gorouter CPU utilization metric.</li>
                    </ol>
                    </td>
        </tr>
</table>

Q: Should we put "suggested starting point is 100ms" in the Red threshold?

Q: What is the metric for "the CPU utilization of the router VMs"? We should reference it above.

<a name="mssincelastregistryupdate"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.ms\_since\_last\_registry\_update<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in milliseconds since the last route register was received.
                        <br><br>
                <strong>Use</strong>: Indicates if routes are not being registered to apps correctly. 
                        If normal platform usage, consider alerting if it has been at least 30 seconds since the 
                        Gorouter last received a message from an app.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ms)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum over the last 5 minutes of <code>gorouter.ms\_since\_last\_registry\_update</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: <em>N/A</em> <br>
                <strong>Red critical</strong>: &gt; 30,000</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Look at the Gorouter and route\_emitter logs to look for connection issues to NATS.</li>
                        <li>Check the BOSH logs to see if the NATS, Gorouter or route\_emitter VMs are failing.</li>
                        <li>Look more broadly at the health of all VMs, particularly Diego-related VMs.</li>
                        <li>If problems persist, pull the Gorouter and route\_emitter logs and contact 
                            Pivotal Support to say there are consistently long delays in route registry.</li>
                    </ol>
                    </td>
        </tr>
</table>

Q: Can we delete "If normal platform usage, consider alerting if it has been at least 30 seconds since the
router last received a message from an app."? It is just a long-winded way to say "<strong>Red critical</strong>: &gt; 30,000". 


<a name="bad_gateways"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.bad\_gateways<br><br></th></tr>
     <tr>
         <th width="25%">Description</th>
            <td>The lifetime (since the Gorouter VM started) number of bad gateways, 502 response from Gorouter itself.<br>
                The Gorouter emits a 502 bad gateway error when it has a route in the routing table 
                and, in attempting to make a connection to the backend, finds that backend not there, that it doesn't exist.
                    <br><br>
                    <strong>Use</strong>: Indicates that route tables might be stale.
                    Stale routing tables suggest an issue in the route register management plane, 
                    which indicates that something has likely changed with the locations of the containers. 
                    Always investigate unexpected increases in this metric.
                    <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Count (Integer, Lifetime)<br>
                 <strong>Frequency</strong>: 5 s<br>
             </td>
     </tr>
     <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 5 minutes of the per second delta of <code>gorouter.bad\_gateways</code></td>
     </tr>
     <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
     </tr>
     <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Look in the Gorouter and route\_emitter logs for connection issues to NATS.</li>
                        <li>Check the BOSH logs to see if the NATS, Gorouter, or route\_emitter VMs are failing.</li>
                        <li>Look broadly at the health of all VMs, particularly Diego-related VMs.</li>
                        <li>If problems persist, pull Gorouter and route\_emitter logs and contact 
                            Pivotal Support to say there has been an unusual increase in Gorouter bad gateway responses.</li>
                    </ol>
                    </td>
     </tr>
</table>

Q: Is there a simple way to say “route register management plane”? e.g. “route registration”?




<a name="responses.5xx"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.responses.5xx<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The lifetime (since the Gorouter VM started) number of requests completed by the component 
                    for HTTP status family 5xx, server errors.
                    <br><br>
                    <strong>Use</strong>: A repeatedly crashing app is often the cause of a big increase in 5xx responses.
                    However, response issues from apps can also cause an increase in 5xx responses.
                    Always investigate an unexpected increase in this metric.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Integer)<br>
                 <strong>Frequency</strong>: 5 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>5-minute average of the per second delta of <code>gorouter.responses.5xx</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Look for out-of-memory errors and other app-level errors.</li>
                        <li>As a temporary measure, ensure that the troublesome app is scaled to more than one instance.</li>
                    </ol>
                    </td>
        </tr>
</table>

Q: Okay that I replaced "this instance of the router" with "since the Router VM started"?

Q: I replaced "various apps on the platform" with just "apps". Okay?

<a name="total_routes"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.total\_routes<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The current total number of routes registered with the Gorouter
                    <br><br>
                    <strong>Use</strong>: Indicates uptake and gives a picture of the overall 
                    growth of the environment for capacity planning.
                    Use this metric to alert you if the number of routes falls outside of the normal
                    range for your deployment.
                    For example, dramatic increases in the total routes outside of expected business 
                    events might point to a denial-of-service attack. 
                    Or, dramatic decreases in this metric volume might indicate a problem with 
                    the route registration process (occurs periodically), such as an
                    app outage or that something in the route register management plane has failed.<br><br>
                    This metric is good to draw attention to a dramatic drop
                    on a dashboard. However, for alerting, trigger alerts from the 
                    <code>gorouter.ms\_since\_last\_registry\_update</code> and not the <code>gorouter.total\_routes</code>
                    metric. 
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>5-minute average of the per second delta of <code>gorouter.bad\_gateways</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>For capacity needs, scale up or down the Gorouter VMs as necessary.</li>
                        <li>For significant drops in current total routes, see the 
                           <a href="#mssincelastregistryupdate"> <code>gorouter.ms\_since\_last\_registry\_update</code></a>
                            metric value for additional context.</li>
                        <li>Look at the Gorouter and route\_emitter logs to look for connection issues to NATS.</li>
                        <li>Check the BOSH logs to see if the NATS, Gorouter or route\_emitter VMs are failing.</li>
                        <li>Look broadly at the health of all VMs, particularly Diego-related VMs.</li>
                        <li>If problems persist, pull the Gorouter and route\_emitter logs and contact Pivotal Support.</li>
                    </ol>
                    </td>
        </tr>
</table>

Q: What is a "router negative case"?

Q: Is my rewrite of this okay? "
                    Regarding monitoring for Gorouter negative cases, Pivotal 
                    recommends that 'gorouter.total\_routes' is a better visualization metric to draw 
                    attention to a dramatic drop via dashboard monitoring, but the metric 
                    'gorouter.ms\_since\_last\_registry\_update' is a superior metric to generate triggered alerts from.:

Q: I changed "scale up or down the gorouter instances" to "scale up or down the router VMs" (We don't mention gorouter instances anywhere else — I assume that they are the same?)


Q: Is there a simple way to say "route register management plane has failed"? e.g. "route registration has failed"?

Q: Because this is for an external audience, I'm replacing all "TEAM-NAME recommends..." with "Pivotal recommends..."

## <a id="doppler-server"></a> Doppler Server Metrics

<a name="listeners.receivedEnvelopes"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>DopplerServer.listeners.receivedEnvelopes<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The total number of messages received across 
                    all of the Doppler’s listeners (UDP, TCP, TLS, and GRPC).
                    <br><br>
                    <strong>Use</strong>: Provides insight into how much traffic is being 
                    handled by the logging system. This metric is an indicator of logging consistency.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Integer)<br>
                 <strong>Frequency</strong>: 5 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum delta per minute of <code>dopplerserver.listeners.receivedEnvelopes</code> over a 5-minute window</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Scale up or down the Firehose log receiver and Dopplers.<br>
                        Recommendation from Firehose team: scaling down might be good for capacity planning, 
                            but generally is not recommended.
                    </ol>
                    </td>
        </tr>
</table>

Q: How to rewrite the response? "Scale up the Firehose log receiver and the Dopplers" 
(How does one do this? provide a link to elsewhere in the doc.)
Why is scaling down not recommended? What can happen? We shouldn't reference internal teams in a customer-facing doc. 

<a name="deriveddopplerserver.doppler"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>DopplerServer.doppler.shedEnvelopes 
                       + DopplerServer.TruncatingBuffer.totalDroppedMessages<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The number of messages dropped per sink: the lifetime (since the VM started) total number of messages 
                    intentionally dropped by Doppler from all of its sinks due to back pressure.<br>
                    The metric `DopplerServer.TruncatingBuffer.totalDroppedMessages` is 
                    being transitioned to `DopplerServer.doppler.shedEnvelopes`. 
                    In PCF v1.10, both metrics are still present and should be combined to present a full picture of dropped messages. 
                    In PCF v1.10, truncating buffer usage is approximately 5% of all possible traffic, primarily syslog drains.
                    <br><br>
                    <strong>Use</strong>: Set an alert to indicate if there is too much traffic coming in to the Dopplers 
                     or if the Firehose consumers are not keeping pace. 
                     Both issues result in dropped messages. This metric is an important indicator of consistent logging.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Integer)<br>
                 <strong>Frequency</strong>: 5 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum delta per minute of <code>derived=(dopplerserver.doppler.shedenvelopes + dopplerServer.TruncatingBuffer.totalDroppedMessages)</code>
                    over a 5-minute window</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 5<br>
                <strong>Red critical</strong>: &ge; 10</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Scale up the Firehose log receiver and Dopplers.
                    </td>
        </tr>
</table>

## <a id="bosh"></a> System (BOSH) Metrics

<a name="healthy"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.healthy<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>`1` means the system is healthy, and `0` means the system is not healthy.
                <br><br>
                <strong>Use</strong>: This is the most important BOSH metric to monitor. 
                 It indicates if the VM emitting the metric is healthy.
                 Review this metric for all VMs to estimate the overall health of the system.
                 <br><br>
                 <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                 <strong>Type</strong>: Gauge (Float, 0-1)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 5 minutes of <code>system.healthy</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: <em>N/A</em> <br>
                <strong>Red critical</strong>: &lt; 1</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Investigate CF logs for the unhealthy component(s).<br>
                        Multiple unhealthy VMs signals problems with the underlying IAAS layer.
                    </td>
        </tr>
</table>


<a name="mem.percent"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.mem.percent<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>System Memory — Percentage of memory used on the VM 
                        <br><br>
                 <strong>Use</strong>: Set an alert and investigate if the free RAM is low over an extended period. 
                 <br><br>
                 <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                 <strong>Type</strong>: Gauge (%)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 10 minutes of <code>system.mem.percent</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 80%<br>
                <strong>Red critical</strong>:&ge; 90%</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    The response depends on the job.
                </td>
        </tr>
</table>

Q: Can we make examples for the recommended responses? If the job is x, then y.

<a name="disk.system.percent"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.disk.system.percent<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>System disk — Percentage of the system disk used on the VM
                        <br><br>
                 <strong>Use</strong>: Set an alert to indicate when the system disk is almost full.
                 <br><br>
                 <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                 <strong>Type</strong>: Gauge (%)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 30 minutes of <code>system.disk.system.percent</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 80%</br>
                <strong>Red critical</strong>: &ge; 90%</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    Investigate what is filling the jobs system partition. <br>
                    This partition should not fill because BOSH deploys jobs to use ephemeral and persistent disks.</li>
                    </td>
        </tr>
</table>


<a name="disk.ephemeral.percent"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.disk.ephemeral.percent<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Ephemeral disk — Percentage of the ephemeral disk used on the VM
                <br><br>
               <strong>Use</strong>: Set an alert and investigate further if the ephemeral 
                disk consumption is too high for a given job over an extended period.
                <br><br>
                <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                <strong>Type</strong>: Gauge (%)<br>
                <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 30 minutes of <code>system.disk.ephermal.percent</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 80%</br>
                <strong>Red critical</strong>: &ge; 90%</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Issue ```bosh vms --details``` to view jobs on affected deployments.</li>
                        <li>Determine cause of the data consumption, and, if appropriate, 
                            increase disk space or scale up the affected jobs.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="disk.persistent.percent"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.disk.persistent.percent<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Persistent disk — Percentage of persistent disk used on the VM
                        <br><br>
                <strong>Use</strong>: Set an alert and investigate further if the persistent disk usage for a job is too high 
                over an extended period.
                        <br><br>
                 <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                 <strong>Type</strong>: Gauge (%)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 30 minutes of <code>system.disk.persistent.percent<code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 80%<br>
                <strong>Red critical</strong>: &ge; 90%</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Issue ```bosh vms --details``` to view jobs on affected deployments.</li>
                        <li>Determine cause of the data consumption, and, if appropriate, 
                            increase disk space or scale up affected jobs.</li>
                    </ol>
                    </td>
        </tr>
</table>

Q: Is there any difference between use and consumption?

Q: How long is an extended period?

Q: Is there any difference between an extended period and a continued period?

Q: Is there any difference between scale out and scale up?

<a name="cpu.user"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.cpu.user<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>CPU utilization — The percentage of CPU spent in user processes
                <br><br>
                <strong>Use</strong>: Set an alert and investigate further if the CPU utilitization is
                too high for a given job.<br>
                For monitoring Gorouter performance, CPU utilization of the Gorouter VM is 
                the recommended key capacity scaling indicator.
                For more information, see <a href="./key-cap-scaling.html#system.cpu.user">Gorouter Latency and Throughput</a>.
                <br><br>
                <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                <strong>Type</strong>: Gauge (%)<br>
                <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 5 minutes of <code>system.cpu.user</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 85%<br>
                <strong>Red critical</strong>: &ge; 95%</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Investigate the cause of the spike.</li>
                        <li>If the cause is a normal workload increase, then scale up the affected jobs.</li>
                    </ol>
                    </td>
        </tr>
</table>

Q: WHAT is a "user process"? 

Q: WHAT is a "given job"? 


