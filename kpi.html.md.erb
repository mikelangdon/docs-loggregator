---
title: Key Performance Indicators 
owner: PCF Metrics Platform Monitoring
---

This topic describes how to use Key Performance Indicators (KPIs) to monitor your Pivotal Cloud Foundry (PCF) 
deployment and ensure it is in a good operational state. 

## <a id="overview"></a> Overview

<table>
   <tr>
      <th width="35%">For metrics related to the component&hellip;</th>
      <th>and the source&hellip;</th>
      <th>see&hellip;</th>
   <tr>
   </tr>
       <td>Diego</td>
       <td>Auctioneer</td>
       <td><a href="#auctioneer">Link.</a></td>
   </tr>
   </tr>
       <td>Diego</td>
       <td>BBS</td>
       <td><a href="#bbs">Link.</a></td>
   </tr>
   </tr>
       <td>Diego</td>
       <td>Cell</td>
       <td><a href="#cell">Link.</a></td>
   </tr>
   </tr>
       <td>Diego</td>
       <td>nsync_bulker</td>
       <td><a href="#nsync_bulker">Link.</a></td>
   </tr>
   </tr>
       <td>Diego</td>
       <td>Router Emitter</td>
       <td><a href="#route_emitter">Link.</a></td>
   </tr>
   </tr>
       <td>Router</td>
       <td>Gorouter</td>
       <td><a href="#gorouter">Link.</a></td>
   </tr>
   </tr>
       <td>Doppler Server</td>
       <td><em>N/A</em></td>
       <td><a href="#doppler-server">Link.</a></td>
   </tr>
   </tr>
       <td>System (BOSH)</td>
       <td><em>N/A</em></td>
       <td><a href="#bosh">Link.</a></td>
   </tr>
</table>

## <a id="auctioneer"></a> Diego Auctioneer Metrics
 
<a name="AuctioneerLRPAuctionsFailed"></a>

<table>
   <tr><th colspan="2" style="text-align: center;"><br> auctioneer.AuctioneerLRPAuctionsFailed<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The cumulative number of LRP instances that the auctioneer failed to place on Diego cells <br><br>
   
      <strong>Use</strong>: This metric can indicates that PCF is out of container space or that there is a lack of resources within your environment.
   
      This error is most common due to capacity issues, for example, if cells do not have 
      enough resources or if cells are blipping in and out of health.
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: Counter, integer<br>
      <strong>Frequency</strong>: During each auction<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Per minute average over a 5 minute window</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &ge; 0.5<br>
      <strong>Red critical</strong>: &ge; 1</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
      Do the following:<br><br>
         <ol>
            <li>Investigate the health of your Diego cells to determine if they
            are the resource type causing the problem.</li> 
            <li>Consider scaling additional cells using Ops Manager.</li>
            <li>If scaling cells does not solve the problem, retrieve logs
            from the brain VM and BBS node and open a ticket with PCF Support indicating that LRP auctions are failing.</li>
         </ol>
      </td>
   </tr>
</table>

<a name="AuctioneerFetchStatesDuration"></a>

<table>
   <tr><th colspan="2" style="text-align: center;"><br>auctioneer.AuctioneerFetchStatesDuration<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td> Time in ns that the auctioneer took to fetch state from all the cells when running its auction.<br><br>

      <strong>Use</strong>: Indicates how the cells themselves are performing. 
      Alerting on this metric helps alert that app stage requests for Diego may be failing.

      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: Gauge, integer in ns<br>
      <strong>Frequency</strong>: During event, during each auction<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td> 5 minute maximum divided by 1,000,000,000</td>
   </tr>
   <tr>
         <th>Recommended alert thresholds</th>
         <td><strong>Yellow warning</strong>: &ge; 5 s<br>
         <strong>Red critical</strong>: &ge; 10 s</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td> Do the following:<br><br>
          <ol>
             <li>Check the health of the cells by reviewing the logs and looking for errors.</li>
             <li>Review IaaS console metrics.</li>
             <li>Collect logs from brain VMs and cells and open ticket with PCF support indicating that fetching cell states is taking too long.</li>
          </ol>
       </td>
   </tr>
</table>

<a name="AuctioneerLRPAuctionsStarted"></a>
<table> 
   <tr><th colspan="2" style="text-align: center;"><br>auctioneer.AuctioneerLRPAuctionsStarted<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Cumulative number of LRP instances that the auctioneer successfully placed on Diego cells. Emitted during each auction.
                        <br><br>
                <strong>Use</strong>: Provides a sense of running system activity levels occurring in your environment. 
                Can also give you a sense of how many AIs have been started over time.
                Suggested alert threshold can help indicate a lot of container churn. 
                However, for capacity planning purposes it may be more interesting to observe deltas over a long time window. 
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Integer)<br>
                 <strong>Frequency</strong>: During event, during each auction<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>per minute average of auctioneer.AuctioneerLRPAuctionsStarted averaged over a 5 minute window</td>
</tr>
<tr>
<th>Recommended alert thresholds</th>
<td><strong>Yellow warning</strong>: Dynamic<br>
<strong>Red critical</strong>: Dynamic</td>
</tr>
<tr>
<th>Recommended response</th>
<td>
When observing a lot of container churn, do the following:<br><br>
    <ol>
    <li>Look to eliminate explainable causes of temporary churn, such as a deploy or a lot a current developer activity</li>
    <li>For extended periods of high or low activity, consider scaling up or down CF components as necessary.</li>
    </ol>
    </td>
</tr>
</table>


<a name="AuctioneerTaskAuctionsFailed"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>auctioneer.AuctioneerTaskAuctionsFailed<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Cumulative number of Tasks that the auctioneer failed to placed on Diego cells. Emitted during each auction.
                        <br><br>
                <strong>Use</strong>: Failing task auctions indicate a lack of resources 
                        within your environment and that you likely need to scale. 
                        This indicator can also increase when the task is requesting an isolation segment, volume drivers, 
                        or stack that is unavailable, either not deployed or lacking in sufficient resources to accept the work.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Float)<br>
                 <strong>Frequency</strong>: During Event<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>per minute average of auctioneer.AuctioneerTaskAuctionsFailed averaged over a 5 minute window</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 0.5 <br>
                <strong>Red critical</strong>: &ge; 1</td>
        </tr>
<tr>
<th>Recommended response</th>
<td>
Do the following:<br><br>
    <ol>
    <li>Investigate the health of Diego cells.</li>
    <li>consider scaling additional cells using Ops Manager.</li>
    <li>If scaling cells doesn't solve the problem, 
                        then get logs from the brain VM and BBS node for troubleshooting, 
                        and open ticket with PCF Support indicating that task auctions are failing.</li>
    </ol>
    </td>
</tr>
</table>

## <a id="bbs"></a> Diego BBS Metrics

<a name="ConvergenceLRPDuration"></a>
<table>
   <tr><th colspan="2" style="text-align: center;"><br>bbs.ConvergenceLRPDuration<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Time in ns that the BBS took to run its LRP convergence pass. 
          Emitted every 30 seconds when LRP convergence runs (emission should be fairly constant on a running Diego-based PCF).
          <br><br>
          <strong>Use</strong>: If the convergence run begins taking too long, 
          it is possible that apps or tasks are crashing and not being restarted. 
          This symptom can also be a proxy for indicating loss of connectivity to the BBS database.
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Integer in ns)<br>
          <strong>Frequency</strong>: During Event<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>15 minute maximum of bbs.ConvergenceLRPDuration divided by 1,000,000,000</td>
   </tr>
      <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &ge; 10 s<br>
          <strong>Red critical</strong>: &ge; 20 s</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          Do the following:<br><br>
          <ol>
              <li>Check Diego BBS logs for errors.</li>
              <li>Try vertically scaling the BBS VM resources up. 
              For example, add more CPUs/memory depending on its system.cpu/system.memory metrics.</li>
              <li>If that does not solve the issue, collect logs from the BBS to provide to Pivotal Support for additional troubleshooting.
          </ol>
          </td>
   </tr>
</table>


<a name="RequestLatency"></a>
<table>
   <tr><th colspan="2" style="text-align: center;"><br>bbs.RequestLatency<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Time in ns that the BBS took to handle requests, aggregated across all its API endpoints. 
      Emitted when the BBS API handles requests (emission should be fairly constant on a running Diego-based PCF).
          <br><br>
          <strong>Use</strong>: If this rises, it indicates that the PCF API is slowing. 
          Response to certain "cf" commands are slow if request latency is high.
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Integer in ns)<br>
          <strong>Frequency</strong>: During Event<br>
    </tr>
    <tr>
      <th>Recommended measurement</th>
      <td>15 minute maximum of bbs.RequestLatency divided by 1,000,000,000</td>
    </tr>
    <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &ge; 5 s<br>
          <strong>Red critical</strong>: &ge; 10 s</td>
    </tr>
    <tr>
      <th>Recommended response</th>
      <td>
          Do the following:<br><br>
          <ol>
              <li>Check CPU and Memory statistics in Ops Manager.</li>
              <li>Check Diego BBS logs faults/errors that could indicate issues with BBS.</li>
              <li>Try scaling the BBS VM resources up. For example, add more CPUs/memory 
                  depending on its system.cpu/system.memory metrics.</li>
              <li>If doesn't solve the issue, then collect a sample of the cell VM logs 
                  from the BBS VMs for Pivotal Support to further troubleshoot.
          </ol>
          </td>
    </tr>
</table>


<a name="Domain.cf-apps"></a>
<table>
   <tr><th colspan="2" style="text-align: center;"><br>bbs.Domain.cf-apps<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Whether the 'cf-apps' domain is up-to-date, so that CF apps from CC have been 
          synchronized with DesiredLRPs for Diego to run. 1 means the domain is up-to-date, 
          no data means it is not. Emitted periodically. 
          <br><br>
          <strong>Use</strong>: cf-apps is used to translate Cloud Controller desired state into Diego implementation. 
          Without freshness, changes in cloud controller state is not guaranteed to propagate through the system. 
          If CC and Diego are out of sync the set of running applications can stray from the set of desired applications.
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: <br>
          <strong>Frequency</strong>: 30 s<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes of bbs.Domain.cf\_apps</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: <em>N/A</em> <br>
      <strong>Red critical</strong>: &lt; 1 </td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          Do the following:<br><br>
          <ol>
              <li>Check the nysnc bulker logs and Diego BBS logs.</li>
              <li>If problem continues, collect logs from the Brain and BBS VMs 
                  and create ticket indicating that cf domain is not being kept fresh.</li>
          </ol>
      </td>
   </tr>
</table>


<a name="LRPsExtra"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>bbs.LRPsExtra<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Total number of LRP instances that are no longer desired but still have a BBS record. 
                        Emitted every 30 seconds.
                        <br><br>
                <strong>Use</strong>: Diego has more LRP running than expected. 
                        When Diego wants to add more apps, the BBS sends a request to the auctioneer 
                        to spin up additional Long Running Processes(LRPs). 
                        LRPsExtra is the total number of LRP instances that are no longer 
                        desired but still have a BBS record. 
                        This could indicate issues with the BBS. 
                        Note that an app delete with many instances can temporarily spike this metric, 
                        however a sustained spike in bbs.LRPsExtra is unusual and should be investigated further. 
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>5 minute average of bbs.LRPsExtra</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 5<br>
                <strong>Red critical</strong>:  &ge; 10</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Review the BBS component logs for proper operation/errors, looking for detailed error messages.</li>
                        <li>If the condition persists, collect Diego BBS logs, and contact Pivotal support.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="LRPsMissing"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>bbs.LRPsMissing<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Total number of LRP instances that are desired but have no record in the BBS. Emitted every 30 seconds.
                        <br><br>
                <strong>Use</strong>: Diego has less LRP running than expected. 
                        When Diego wants to add more apps, the BBS sends a request to the auctioneer 
                        to spin up additional Long Running Processes(LRPs). 
                        LRPsMissing is the total number of LRP instances that are desired but have no BBS record. 
                        This could indicate issues with the BBS. 
                        Note that an app push with many instances can temporarily spike this metric, 
                        however a sustained spike in bbs.LRPsMissing is unusual and should be investigated further.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>5 minute average of bbs.LRPsMissing</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 5<br>
                <strong>Red critical</strong>: &ge; 10</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Review the BBS component logs for proper operation/errors, looking for detailed error messages.</li>
                        <li>If the condition persists, collect Diego BBS logs, and contact Pivotal support.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="bbs.CrashedActualLRPs"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>bbs.CrashedActualLRPs<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Total number of LRP instances that have crashed. Emitted every 30 seconds.
                <br><br>
                <strong>Use</strong>: Indicates how many instances in the deployment are in a crashed state. 
                An increase in bbs.CrashedActualLRPs can indicate several problems, from a bad app with many instances associated, 
                to a platform issue that is resulting in app crashes. 
                This metric is most helpful to create a baseline in a deployment. 
                Once a baseline is created, a deployment specific alert can be created to notify of a spike in crashes above trend line. 
                Alert values should be tuned to the deployment. 
                <br><br>
                <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over last 5 minutes of bbs.CrashedActualLRPs</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Look at BBS logs for apps that are crashing and their respective cells for indicators of app causation.</li>
                        <li>Pull BBS logs.</li>
                        <li>If clear from a particular set of cells, the cell logs are helpful.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="1hraverageofbbs.LRPsRunning"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>derived=(1hr average of bbs.LRPsRunning - prior 1hr average of bbs.LRPsRunning)<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Rate of change in app instances being started or stopped on the platform. 
                    Derived from bbs.LRPsRunning, which is emitted periodically 
                    (emission should be fairly constant on a running Diego-based PCF), 
                    and represents the total number of LRP instances that are running on cells.
                    <br><br>
                    <strong>Use</strong>: Delta reflects upward or downward trend for AIs started or stopped. 
                    Helps to provide a picture of the overall growth trend of the environment for capacity planning. 
                    May want to alert on delta values out of expected range.
                    <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: During event<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>derived=(1hr average of bbs.LRPsRunning - prior 1hr average of bbs.LRPsRunning)</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                   Do the following:<br><br>
                   <ol>
                     <li>Scale up down CF components as necessary.</li>
                   </ol>
                    </td>
        </tr>
</table>



## <a id="cell"></a> Diego Cell Metrics

<a name="rep.CapacityRemainingMemory"></a>
<table>
   <tr><th colspan="2" style="text-align: center;"><br>rep.CapacityRemainingMemory<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Remaining amount in MiB of memory available for this cell to allocate to containers. Emitted every 30 seconds.
          <br><br>
          <strong>Use</strong>: Indicates the available cell memory. 
          If low memory, this could prevent app scaling and new deployments. 
          It is common to look at this metric as both a minimum value as well as a sum capacity across all cells. 
          Minimum value over time of this metric has more informational value than alerting value. 
          It can be an interesting heatmap, showing average variance and/or density over time. 
          Sum capacity can also be an interesting indicator of the need to potentially scale an environment. 
          However, the greater operational value is to understand a deployments average app size, 
          and monitor/alert on ensuring that at least some cells have large enough capacity to accept standard app size pushes. 
          For example, if pushing a 4&nbsp;GB app, Diego would have trouble placing that app if there is no one cell with sufficient capacity. 
          If deployment average was 3.9&nbsp;GB free, yet no one cell contained at least 4&nbsp;GB, the app push would fail. 
          As an example, Pivotal Cloud Ops uses a standard of 4&nbsp;GB, and computes and monitors 
          for the number of cells with at least 4&nbsp;GB free. 
          When the number of cells with at least 4&nbsp;GB falls below a defined threshold, 
          this is a scaling indicator alert to add additional capacity. 
          This "Free 4GB Chunk" count threshold should be tuned to the deployment size 
          and the standard size of apps being pushed to the platform.<br>
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Integer in bytes)<br>
          <strong>Frequency</strong>: 30 s<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>The minimum amount of memory in a 30 minute window that the cell can allocate to containers.<br>
          <em>Implementation Tip</em>: Iterate through each Diego cell and grab the rep.CapacityRemainingMemory metric.
          Divide this metric by 1000 to get the value in GB. 
          Then compare it to your minimum capacity threshold, and count the number of cells that have enough free space.</td>
  </tr>
  <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Dynamic<br>
          <strong>Red critical</strong>: Dynamic</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          Do the following:<br><br>
          <ol>
              <li>Assign more resources to the cells or assign more cells.</li>
                        <li>Scale additional Diego Cells using Ops Manager.</li>
                    </ol>
                    </td>
        </tr>
</table>

<a name="rep.CapacityRemainingMemory2"></a>
<table>
   <tr><th colspan="2" style="text-align: center;"><br>rep.CapacityRemainingMemory<br>
       Alternative use.<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Remaining amount in MiB of memory available for this cell to allocate to containers. Emitted every 30 seconds.
          <br><br>
          <strong>Use</strong>: Can be used to indicate low memory capacity overall in the platform. 
          Low memory can prevent application scaling and new deployments. 
          Sum capacity overall is helpful to understanding the need to take action to scale the platform. 
          Observing how capacity consumption trends over time is useful for on-going capacity planning.
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Integer in bytes)<br>
          <strong>Frequency</strong>: 30 s<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Sum of the total available memory capacity across across all cells for the past 5 minutes<br>
             5 minute sum of rep.CapacityRemainingMemory divided by 0.000001024</td>
  </tr>
  <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 32<br>
          <strong>Red critical</strong>: &le; 16</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          Do the following:<br><br>
          <ol>
              <li>Assign more resources to the cells or assign more cells</li>
              <li>Scale additional Diego Cells via Ops Manager</li>
          </ol>
                    </td>
        </tr>
</table>
<a name="CapacityRemainingDisk"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br> rep.CapacityRemainingDisk<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Remaining amount in MiB of disk available for this cell to allocate to containers. Emitted every 30 seconds.
                    <br><br>
                    <strong>Use</strong>: Indicates the available cell disk. 
                Low disk capacity could prevent app scaling and new deployments.
                Similar to rep.CapacityRemainingMemory, it can be meaningful to assess how many chunks of free disk space are above a given threshold. 
                As Diego staging tasks can fail without at least 4GB free, this recommend red threshold is based on the minimum disk capacity 
                across the deployment falling below 4GB in the prior 5 minutes.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Integer in bytes)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>5 minute minimum rep.CapacityRemainingDisk divided by 1024 (across all instances)</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &le; 6<br>
                <strong>Red critical</strong>:&le; 3.5 </td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Assign more resources to the cells or assign more cells</li>
                        <li>Scale additional Cells using Ops Manager.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="RepBulkSyncDuration"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>rep.RepBulkSyncDuration<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in ns that the cell rep took to synchronize the ActualLRPs it has claimed with its actual garden containers. 
                        <br><br>
                <strong>Use</strong>: Sync times that are too high may indicate issues with the BBS.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge(Float in ns)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>15 minute maximum of rep.RepBulkSyncDuration divided by 1,000,000,000</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 10 s<br>
                <strong>Red critical</strong>: &ge; 20 s </td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Investigate Diego BBS Logs for faults/errors.</li>
                        <li>If a particular cell or cells look to be problematic, 
                            pull logs for that cell, as well as the BBS logs prior to contacting Pivotal Support.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="UnhealthyCell"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>rep.UnhealthyCell<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The cell  periodically checks its health against the garden backend. 
                    When it fails this healthcheck, it emits a 1 (unhealthy).
                    For Diego cells, 0 signifies healthy, and 1 signifies unhealthy.
                    <br><br>
                    <strong>Use</strong>: Should alert for further investigation if 
                    multiple unhealthy Diego cells are detected in the given time window. 
                    If one cell is impacted, it does not participate in auctions, 
                    but customer impact is usually low. If multiple cells are impacted, 
                    this can indicate a larger problem with Diego.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge(Float, 0-1)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Max over last 5 minutes of rep.UnhealthyCell</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: <em>N/A</em><br>
                <strong>Red critical</strong>: &gt; 1 </td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Investigate Diego Cell Servers for faults/errors.</li>
                        <li>If a particular cell or cells look to be problematic, 
                            pull logs for that cell, as well as the BBS logs prior to contacting Pivotal Support.</li>
                    </ol>
                    </td>
        </tr>
</table>


## <a id="nsync_bulker"></a> Diego nsync_bulker Metrics

<a name="DesiredLRPSyncDuration"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>nsync\_bulker.DesiredLRPSyncDuration<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in ns that the nsync-bulker took to synchronize CF apps and Diego DesiredLRPs. Emitted every 30 seconds.
                    <br><br>
                    <strong>Use</strong>: Cloud Controller and Diego Brain should be kept synchronized. 
                    This symptom can be a proxy for indicating the BBS database is in an unhealthy state. 
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ns)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>15 minute maximum of nsync\_bulker.DesiredLRPSyncDuration divided by 1,000,000,000</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 10 s<br>
                <strong>Red critical</strong>: &ge; 20 s</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Investigate Diego BBS metrics and logs and cc-bridge log files for errors</li>
                    </ol>
                    </td>
        </tr>
</table>

##<a id="route_emitter"></a>Diego Router Emitter Metrics

<a name="RouteEmitterSyncDuration"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>route\_emitter.RouteEmitterSyncDuration<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in ns that the active route-emitter took to perform its synchronization pass. 
                    Emitted every 30 seconds.
                    <br><br>
                    <strong>Use</strong>: Increases in this metric indicate that the route emitter may be having 
                    trouble maintaining an accurate routing table to broadcast to the gorouters. 
                    Alerting values need to be tuned per the deployment, based on historical data adjusted based on observations over time. 
                    The suggested starting point is &ge; 10 for the yellow threshold and &ge; 20 for the critical threshold. 
                    Pivotal has observed on Pivotal Web Services that above 20 s, the BBS may be failing.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ns)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>15 minute maximum of route\_emitter.RouteEmitterSyncDuration divided by 1,000,000,000</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Investigate routing table and Diego brains for errors.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="ConsulDownMode"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>route\_emitter.ConsulDownMode<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Emits 0 or 1 health status periodically (emission should be fairly constant on a running Diego-based PCF) 
                    0 = healthy  1 = route emitter detects consul is down 
                    <br><br>
                    <strong>Use</strong>: Can indicate consul down during Installation/Upgrade 
                    Loss of the consul cluster would result in app becoming unavailable 
                    as their routes were pruned from the routing table
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float, 0-1)<br>
                 <strong>Frequency</strong>: Periodically<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over last 5 minutes of route\_emitter.ConsulDownMode</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: <em>N/A</em><br>
                <strong>Red critical</strong>: Not equal to 1 </td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>During upgrade, reduce the consul server nodes to 1, and then upgrade.</li>
                        <li>Outside of installation/update, consider disaster recovery for consul.</li>
                    </ol>
                    </td>
        </tr>
</table>


## <a id="gorouter"></a> Router Gorouter Metrics

<a name="total_requests"></a>
<table>
   <tr><th colspan="2" style="text-align: center;"><br>gorouter.total\_requests<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Lifetime number of requests completed by component since this instance of the router has started
          <br><br>
          <strong>Use</strong>: This metric can provide insight into the overall traffic flow through a PCF foundation. 
          For performance and capacity management, it is most useful to consider this metric a measure of router throughput, 
          and convert it as "Requests Per Second", by deriving 'per\_second(sum:gorouter.total\_requests{*})'. 
          This allows for observed trends in throughput rate that may indicate a need to scale the gorouter.
          Alert values on this metric should be tuned to the specifics of the deployment based on historical observed trends. 
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Counter (Integer)<br>
          <strong>Frequency</strong>: 5 s<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>average over the last 5 minutes of the per/sec calculation of gorouter.total\_requests</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Dynamic<br>
          <strong>Red critical</strong>: Dynamic</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>For the best optimization of the router, this "Requests Per Second" 
          derived throughput metric should be considered in context of router 
          latency and router VM CPU utilization.  From performance and load testing 
          of the gorouter, it has been observed that at approximately 2500 requests 
          per second, latency can begin to increase. If an operator wants to increase 
          throughput, while keeping latency low, it is suggested to scale the gorouters, 
          either horizontally or vertically, while keeping an eye to the suggested 
          gorouter CPU utilization metric (maximum 60-70% CPU utilization range).</td>
   </tr>
</table>


<a name="latency"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.latency<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in milliseconds that the Gorouter took to handle requests to its app endpoints. 
                    This is the average round trip response time to an app, which includes router handling. 
                    Emitted per router request (emission should be fairly constant on a running Diego-based PCF). 
                    <br><br>
                    <strong>Use</strong>: Indicator of how router jobs in PCF may be impacting overall app responsiveness. 
                    Latencies above 100ms could indicate problems with the network, misbehaving apps, 
                    or the need to scale the router itself due to on-going traffic congestion. 
                    Though a suggested starting point is 100ms, an alert value on this metric 
                    should be tuned to the specifics of the given deployment.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ms)<br>
                 <strong>Frequency</strong>: Event<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average of gorouter.latency over last 30 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>Extended periods of high latency can point to several factors. 
                    The router latency measure includes network and app latency impacts as well.<br>
                    Do the following:<br><br>
                    <ol>
                        <li>First inspect logs for possible network issues, or indicators of misbehaving apps.</li>
                        <li>If it appears the router may need to scale due to on-going traffic congestion, 
                            it is not recommended to scale on the latency metric alone. 
                            Instead, look to the CPU utilization of the router VMs.</li>
                        <li>It is suggested to keep CPU utilization within a maximum 60-70% range.</li>
                        <li>Resolve high utilization by scaling the gorouter.</li>
                        <li>If an operator wants to increase throughput, while keeping latency low, 
                            it is suggested to scale the gorouters, either horizontally or vertically, 
                            while keeping an eye to the suggested gorouter CPU utilization metric.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="ms_since_last_registry_update"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.ms\_since\_last\_registry\_update<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in millisecond since the last route register has been been received. Emitted every 30 seconds
                        <br><br>
                <strong>Use</strong>: Can indicate that routes are not being registered to apps correctly. 
                        If normal platform usage, consider alerting if it has been at least 30 seconds since the 
                        router last received a message from an app.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ms)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>maximum over last 5 minutes of gorouter.ms_since_last_registry_update</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: <em>N/A</em> <br>
                <strong>Red critical</strong>: &gt; 30,000</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Look at the gorouter and route_emitter logs to look for connection issues to NATS.</li>
                        <li>Check bosh logs to see if the NATS, gorouter or route_emitter VMs are failing.</li>
                        <li>Then look more broadly at the health of all VMs, particularly Diego related.</li>
                        <li>If problems persist, pull gorouter and route_emitter logs, and contact 
                            Pivotal support to say there are consistently long delays in route registry.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="bad_gateways"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.bad_gateways<br><br></th></tr>
     <tr>
         <th width="25%">Description</th>
            <td>Lifetime number of bad gateways (502 response from router itself) 
                    since this instance of the router has started. Emitted every 5 seconds.
                    <br><br>
                    <strong>Use</strong>: The gorouter emits a 502 bad gateway error 
                    when it has a route in the routing table and the attempt to make a 
                    connection to the backend finds that backend not there (i.e. doesn't exist). 
                    This indicates potential stale routing tables. 
                    Stale routing tables further point to an issue in the route register management plane, 
                    which indicates that something has likely changed with the location of the containers. 
                    Unexpected increases in this metric should be investigated more deeply.
                    <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Count (Integer, Lifetime)<br>
                 <strong>Frequency</strong>: 5 s<br>
             </td>
     </tr>
     <tr>
                <th>Recommended measurement</th>
                <td>5 minute average of per second delta of gorouter.bad_gateways</td>
     </tr>
     <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
     </tr>
     <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Look at the gorouter and route_emitter logs to look for connection issues to NATS.</li>
                        <li>Check bosh logs to see if the NATS, gorouter or route_emitter VMs are failing.</li>
                        <li>Then look more broadly at the health of all VMs, particularly Diego related.</li>
                        <li>If problems persist, pull gorouter and route_emitter logs, and contact 
                            Pivotal support to say there has been an unusual increase in router bad gateway responses.</li>
                        <li>step</li>
                    </ol>
                    </td>
     </tr>
</table>


<a name="responses.5xx"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.responses.5xx<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Lifetime number of requests completed by component since this instance of the 
                    router has started for http status family 5xx (Server Errors)
                    <br><br>
                    <strong>Use</strong>: The more common cause of a large increase in 5xx responses 
                    is a repeatedly crashing app. Could also be indicative of response issues from 
                    various apps on the platform. Therefore a unexpectedly high increase 
                    in this metric should be investigated more deeply.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Integer)<br>
                 <strong>Frequency</strong>: 5 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>5 minute average of per second delta of gorouter.responses.5xx</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Look for Out Of Memory errors or other app level errors.</li>
                        <li>As a temporary band-aid, ensure the troublesome app is scaled beyond one instance.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="total_routes"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.total_routes<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Current total number of routes registered with gorouter in a PCF install
                    <br><br>
                    <strong>Use</strong>: Indicates uptake, and gives a picture of the overall 
                    growth of the environment for capacity planning.  May also be leveraged 
                    as an alert mechanism for extreme high/low variance when deployment standard ranges are known. 
                    For example, dramatic increases in total routes outside of expected business 
                    events may point to possible denial of service attacks. 
                    Or dramatic decreases in this metric volume may indicate a problem with 
                    the route registration process (occurs periodically), indicating a potential 
                    app outage and/or that something in the route register management plane may have failed. 
                    Regarding monitoring for router negative cases, the Router component team 
                    recommends that 'gorouter.total_routes' is a better visualization metric to draw 
                    attention to a dramatic drop via dashboard monitoring, but the metric 
                    'gorouter.ms_since_last_registry_update' is a superior metric to generate triggered alerts from.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>5 minute average of per second delta of gorouter.bad_gateways</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>For capacity needs, scale up or down the gorouter instances as necessary.</li>
                        <li>For significant drops in current total routes, see the 
                            <code>gorouter.ms\_since\_last\_registry\_update</code> metric value for additional context.</li>
                        <li>Look at the gorouter and route\_emitter logs to look for connection issues to NATS.</li>
                        <li>Then check bosh logs to see if the NATS, gorouter or route\_emitter VMs are failing.</li>
                        <li>Then look more broadly at the health of all VMs, particularly Diego related.</li>
                        <li>If problems persist, pull gorouter and route\_emitter logs and contact Pivotal support.</li>
                    </ol>
                    </td>
        </tr>
</table>

## <a id="doppler-server"></a> Doppler Server Metrics

<a name="listeners.receivedEnvelopes"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>DopplerServer.listeners.receivedEnvelopes<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td># messages received by doppler. Total number of messages received across 
                    all of Dopplers listeners (UDP, TCP, TLS, GRPC).
                    <br><br>
                    <strong>Use</strong>: Provides insight into how much traffic is being 
                    handled by the logging system. This serves as an important indicator for consistent logging.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Integer)<br>
                 <strong>Frequency</strong>: 5 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum delta per minute dopplerserver.listeners.receivedEnvelopes over a 5 minute window</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Scale up or down the Firehose log receiver and dopplers.</li>
                        <li>Recommendation from Firehose team: scaling down might be good for capacity planning, 
                            but generally is not recommended</li>
                    </ol>
                    </td>
        </tr>
</table>

<a name="deriveddopplerserver.doppler"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>derived=(DopplerServer.doppler.shedEnvelopes 
                       + DopplerServer.TruncatingBuffer.totalDroppedMessages)<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>number of messages dropped per sink. Lifetime (since VM start) total number of messages 
                    intentionally dropped by Doppler from all of its sinks due to back pressure.
                    The metric `DopplerServer.TruncatingBuffer.totalDroppedMessages` is 
                    being transitioned to `DopplerServer.doppler.shedEnvelopes`. 
                    In PCF v1.10, both metrics are still present and should be combined to present a full picture of dropped messages. 
                    In PCF v1.10, truncating buffer usage is approximately 5% of all possible traffic, primarily syslog drains.
                    <br><br>
                    <strong>Use</strong>: Indicates that there is too much ingress to the dopplers 
                    (i.e. too much traffic coming in), and/or that the Firehose consumers are not keeping pace; 
                    both result in dropped messages. This serves as an important indicator for consistent logging.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Integer)<br>
                 <strong>Frequency</strong>: 5 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum delta per minute derived=(dopplerserver.doppler.shedenvelopes + dopplerServer.TruncatingBuffer.totalDroppedMessages) 
                    over a 5-minute window</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 5<br>
                <strong>Red critical</strong>: &ge; 10</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Scale up the Firehose log receiver and dopplers.</li>
                    </ol>
                    </td>
        </tr>
</table>

## <a id="bosh"></a> System (BOSH) Metrics

<a name="healthy"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.healthy<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>`1` means the system is healthy, and `0` means the system is not healthy
                <br><br>
                <strong>Use</strong>: The most important Bosh Metric to monitor, it indicates whether a 
                given VM emitting this metric is healthy, and points towards the overall health status of your system.
                 <br><br>
                 <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                 <strong>Type</strong>: Gauge (Float, 0-1)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over last 5 minutes of system.healthy</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: <em>N/A</em> <br>
                <strong>Red critical</strong>: &lt; 1</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Investigate CF logs for the unhealthy component(s). 
                        Multiple unhealthy VM signals can point to problems with the underlying IAAS layer.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="mem.percent"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.mem.percent<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>System Memory % - Percentage of memory used on the VM 
                        <br><br>
                <strong>Use</strong>: Indicates there is a continued period of low free RAM                 <br><br>
                 <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                 <strong>Type</strong>: Gauge (%)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over last 10 minutes of system.mem.percent</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 80 <br>
                <strong>Red critical</strong>:&ge; 90</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>It can depend on the job the metric is associated with.</li>
                        <li>If appropriate scale affected jobs out and monitor for improvement.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="disk.system.percent"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.disk.system.percent<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>System Disk % - Percentage of system disk used on the VM
                        <br><br>
                <strong>Use</strong>: Indicates that system disk is almost full                 <br><br>
                 <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                 <strong>Type</strong>: Gauge (%)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over last 30 minutes of system.disk.system.percent</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 80</br>
                <strong>Red critical</strong>: &ge; 90</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Investigate what is filling the Jobs system partition. 
                        This partition should typically not fill as BOSH deploys jobs to use ephemeral and persistent disks.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="disk.epheremal.percent"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.disk.epheremal.percent<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Ephemeral Disk % - Percentage of ephemeral disk used on the VM
                <br><br>
               <strong>Use</strong>: Alert for further investigation if ephemeral 
                disk consumption is too high for a given job for an extended period.
                <br><br>
                <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                <strong>Type</strong>: Gauge (%)<br>
                <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over last 30 minutes of system.disk.ephermal.percent</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 80</br>
                <strong>Red critical</strong>: &ge; 90</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Issue ```bosh vms --details``` to view jobs on affected deployments.</li>
                        <li>Determine cause of the data consumption, and, if appropriate, 
                            either increase disk space or scale affected jobs out.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="disk.persistent.percent"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.disk.persistent.percent<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Persistent Disk % - Percentage of persistent disk used on the VM
                        <br><br>
                <strong>Use</strong>: Alert for further investigation if persistent disk consumption is too high 
                for a given job for an extended period.
                        <br><br>
                                 <br><br>
                 <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                 <strong>Type</strong>: Gauge (%)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over last 30 minutes of system.disk.persistent.percent</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 80<br>
                <strong>Red critical</strong>: &ge; 90</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Do the following:<br><br>
                    <ol>
                        <li>Issue ```bosh vms --details``` to view jobs on affected deployments.</li>
                        <li>Determine cause of the data consumption, and, if appropriate, 
                            either increase disk space or scale affected jobs out.</li>
                    </ol>
                    </td>
        </tr>
</table>


<a name="cpu.user"></a>
<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.cpu.user<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>CPU Utilization (%) - Amount of CPU spent in user processes
                <br><br>
                <strong>Use</strong>: Alert for further investigation if CPU Utilization is 
                too high for a given job.
                For monitoring of router performance, CPU utilization of the Router VM is 
                recommended as a key capacity scaling indicator.
                <br><br>
                <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                <strong>Type</strong>: Gauge (%)<br>
                <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over last 5 minutes of system.cpu.user</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 85<br>
                <strong>Red critical</strong>: &ge; 95</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                  High CPU Utilization Do the following:<br><br>
                    <ol>
                        <li>Investigate cause of spike.</li>
                        <li>Possibly scale affected job if cause is normal workload increase.</li>
                    </ol>
                    </td>
        </tr>
</table>




